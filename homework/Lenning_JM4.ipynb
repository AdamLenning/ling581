{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1\n",
    "Assume the following likelihoods for each word being part of a positive or negative movie review, and equal prior probabilities for each class.\n",
    "\n",
    "        pos     neg\n",
    "I       0.09    0.16\n",
    "always  0.07    0.06\n",
    "like    0.29    0.06\n",
    "foreign 0.04    0.15\n",
    "films   0.08    0.11\n",
    "\n",
    "What class will Naive bayes assign to the sentence “I always like foreign films.”?\n",
    "\n",
    "P(“I always like foreign films”|+) = 0.09 × 0.07 × 0.29 × 0.04 × 0.08 = 0.0000058464\n",
    "P(“I always like foreign films”|-) = 0.16 × 0.06 × 0.06 × 0.15 × 0.11 = 0.0000095040\n",
    "\n",
    "Assigned negative"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 \n",
    "Given the following short movie reviews, each labeled with a genre, either comedy or action:\n",
    "\n",
    "1. fun, couple, love, love- comedy\n",
    "2. fast, furious, shoot- action\n",
    "3. couple, fly, fast, fun, fun- comedy\n",
    "4. furious, shoot, shoot, fun- action\n",
    "5. fly, fast, shoot, love- action\n",
    "\n",
    "and a new document D:\n",
    "\n",
    "fast, couple, shoot, fly\n",
    "\n",
    "compute the most likely class for D. Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "docs = [['fun', 'couple', 'love', 'love'],['fast', 'furious', 'shoot'],['couple' , 'fly', 'fast', 'fun', 'fun'],['furious', 'shoot', 'shoot', 'fun'], ['fly', 'fast', 'shoot', 'love']]\n",
    "genres = ['comedy', 'action', 'comedy', 'action', 'action']\n",
    "D = list(zip(docs, genres))\n",
    "C = Counter(elem[1] for elem in D)\n",
    "\n",
    "\n",
    "def train_naive_bayes(D, C, binary=False):    \n",
    "        \"\"\"trains a naive bayes model on list of documents\n",
    "\n",
    "        Args:\n",
    "            D (list of lists): list of documents to train on\n",
    "            C (Counter/dictionary): dictionary of key = class, and value = occurences of class\n",
    "            binary (bool, optional): whether to run binarized Naive Bayes. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            dict, dict, list: returns the logprior for the classes, the loglikelihood of each word given a class, and the vocab\n",
    "        \"\"\"\n",
    "        logprior = dict()\n",
    "        bigdoc = dict() \n",
    "        loglikelihood = dict()\n",
    "\n",
    "        for c in C: # Calculate P(c) terms\n",
    "                n_doc = len(D)\n",
    "                n_c = C[c]\n",
    "                logprior[c] = np.log(n_c / n_doc)\n",
    "                vocab = [item for sublist in docs for item in sublist]\n",
    "\n",
    "                bigdoc[c] = []\n",
    "                for d in D:\n",
    "                        if d[1] == c:\n",
    "                                if binary:\n",
    "                                        bigdoc[c].extend(np.unique(np.array(d[0])))\n",
    "                                else:\n",
    "                                        bigdoc[c].extend(d[0])\n",
    "\n",
    "                counts = Counter(bigdoc[c])\n",
    "\n",
    "                for word in vocab: # Calculate P(w|c) terms\n",
    "                        count_w_c = counts[c]\n",
    "                        if word not in loglikelihood.keys():\n",
    "                                loglikelihood[word] = dict() # creating new dict entirely for each class\n",
    "                        loglikelihood[word][c] = np.log((count_w_c + 1) / (len(bigdoc[c]) + len(vocab)))\n",
    "                \n",
    "        \n",
    "        return logprior, loglikelihood, vocab\n",
    "\n",
    "\n",
    "def test_naive_bayes(testdoc, logprior, loglikelihood, C, V):\n",
    "        \"\"\"predicts class of testdoc given Naive Bayes trained data\n",
    "\n",
    "        Args:\n",
    "            testdoc (list): list of words\n",
    "            logprior (dict): the logprior for classes\n",
    "            loglikelihood (dict): the loglikelihood of each word given a class\n",
    "            C (Counter/dict): list of classes and counts for each class\n",
    "            V (list): vocabulary\n",
    "\n",
    "        Returns:\n",
    "            [str]: the predicted class\n",
    "        \"\"\"\n",
    "        tot = dict()\n",
    "        for c in C:\n",
    "                tot[c] = logprior[c]\n",
    "                for word in testdoc:\n",
    "                        if word in V:\n",
    "                                tot[c] = tot[c] + loglikelihood[word][c]\n",
    "        return max(tot, key=tot.get)\n",
    "\n",
    "\n",
    "logprior, loglikelihood, vocab = train_naive_bayes(D, C)\n",
    "class_pred = test_naive_bayes(['fast', 'couple', 'shoot', 'fly'], logprior, loglikelihood, C, vocab)\n",
    "print(class_pred)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "action\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3\n",
    "\n",
    "Train two models, multinomial naive Bayes and binarized naive Bayes, both with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted.\n",
    "\n",
    "doc     “good”  “poor”  “great” (class)\n",
    "d1.     3       0       3       pos\n",
    "d2.     0       1       2       pos\n",
    "d3.     1       3       0       neg\n",
    "d4.     1       5       2       neg\n",
    "d5.     0       2       0       neg\n",
    "\n",
    "Use both naive Bayes models to assign a class (pos or neg) to this sentence:\n",
    "\n",
    "A good, good plot and great characters, but poor acting.\n",
    "\n",
    "Recall from page 6 that with naive Bayes text classification, we simply ignore (throw out) any word that never occurred in the training document. (We don’t throw out words that appear in some classes but not others; that’s what add-one smoothing is for.) Do the two models agree or disagree?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "# Create training documents\n",
    "d1 = ['good']*3\n",
    "d1.extend(['great']*3)\n",
    "d2 = ['poor', 'great', 'great']\n",
    "d3 = ['good']\n",
    "d3.extend(['poor']*3)\n",
    "d4 = ['good']\n",
    "d4.extend(['poor']*5)\n",
    "d4.extend(['great']*2)\n",
    "d5 = ['poor', 'poor']\n",
    "\n",
    "# Create input data for training algorithm\n",
    "docs = [d1, d2, d3, d4, d5]\n",
    "classes = ['pos', 'pos', 'neg', 'neg', 'neg']\n",
    "D = list(zip(docs, classes))\n",
    "C = Counter(elem[1] for elem in D)\n",
    "\n",
    "# Run Multinomial Naive Bayes on testdoc\n",
    "multi_logprior, multi_loglikelihood, multi_vocab = train_naive_bayes(D, C)\n",
    "multi_class_pred = test_naive_bayes('A good good plot and great characters but poor acting'.split(sep=' '), multi_logprior, multi_loglikelihood, C, multi_vocab)\n",
    "print(\"multinomial prediction:\", multi_class_pred)\n",
    "\n",
    "# Run Binarized Naive Bayes on testdoc\n",
    "bin_logprior, bin_loglikelihood, bin_vocab = train_naive_bayes(D, C, binary=True)\n",
    "bin_class_pred = test_naive_bayes('A good good plot and great characters but poor acting'.split(sep=' '), bin_logprior, bin_loglikelihood, C, bin_vocab)\n",
    "print(\"binary prediction:\", bin_class_pred)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "multinomial prediction: pos\n",
      "binary prediction: neg\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "2c3deb8cd993c689b061872ff910ef58aea191cabc6baada44a3fe0a4ea922df"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
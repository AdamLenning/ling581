{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JM Chapter 7\n",
    "\n",
    "1. Write out the computations of the remaining three input sets in the second paragraph of 7.2.1 in the reading. Make this a comment (or a markdown cell, etc. in a notebook) at the top of your script.\n",
    "\n",
    "[0, 1]- <br>\n",
    "$ x_1 = 0; x_2 = 1 $ <br>\n",
    "$ h_1= x_1 + x_2 = 0 + 1 = 1 $ <br>\n",
    "$ h_2 = x_1 + x_2 - 1 = 0 + 1 - 1 = 0 $ <br>\n",
    "$ y_1 = h_1 - 2*h_2 = 1 - 2 * 0 = 1 $ <br>\n",
    "\n",
    "[1, 0]- <br>\n",
    "$ x_1 = 1; x_2 = 0 $ <br>\n",
    "$ h_1= x_1 + x_2 = 1 + 0 = 1 $ <br>\n",
    "$ h_2 = x_1 + x_2 - 1 = 1 + 0 - 1 = 0 $ <br>\n",
    "$ y_1 = h_1 - 2*h_2 = 1 - 2 * 0 = 1 $ <br> \n",
    "\n",
    "[1, 1]- <br>\n",
    "$ x_1 = 1; x_2 = 1 $ <br>\n",
    "$ h_1= x_1 + x_2 = 1 + 1 = 2 $ <br>\n",
    "$ h_2 = x_1 + x_2 - 1 = 1 + 1 - 1 = 1 $ <br>\n",
    "$ y_1 = h_1 - 2*h_2 = 2 - 2 * 1 = 0 $ <br>\n",
    "\n",
    "\n",
    "2. Read both of the following tutorials, which are implementations of the language model introduced in 7.5 using two different deep learning frameworks: pytorch and tensorflow. Actually implement at least one of them in your homework. Do your best to understand what the code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/alenning/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alenning/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15667"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "len(brown.paras())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12681\n"
     ]
    }
   ],
   "source": [
    "num_train = 12000\n",
    "UNK_symbol = \"<UNK>\"\n",
    "vocab = set([UNK_symbol])\n",
    "\n",
    "# create brown corpus again with all words\n",
    "# no preprocessing, only lowercase\n",
    "brown_corpus_train = []\n",
    "for idx,paragraph in enumerate(brown.paras()):\n",
    "    if idx == num_train:\n",
    "        break\n",
    "    words = []\n",
    "    for sentence in paragraph:\n",
    "        for word in sentence:\n",
    "            words.append(word.lower())\n",
    "    brown_corpus_train.append(words)\n",
    "\n",
    "# create term frequency of the words\n",
    "words_term_frequency_train = {}\n",
    "for doc in brown_corpus_train:\n",
    "    for word in doc:\n",
    "        # this will calculate term frequency\n",
    "        # since we are taking all words now\n",
    "        words_term_frequency_train[word] = words_term_frequency_train.get(word,0) + 1\n",
    "\n",
    "# create vocabulary\n",
    "for doc in brown_corpus_train:\n",
    "    for word in doc:\n",
    "        if words_term_frequency_train.get(word,0) >= 5:\n",
    "            vocab.add(word)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(872823, 2)\n",
      "(872823, 1)\n",
      "(174016, 2)\n",
      "(174016, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# create required lists\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "\n",
    "# create word to id mappings\n",
    "word_to_id_mappings = {}\n",
    "for idx,word in enumerate(vocab):\n",
    "    word_to_id_mappings[word] = idx\n",
    "\n",
    "# function to get id for a given word\n",
    "# return <UNK> id if not found\n",
    "def get_id_of_word(word):\n",
    "    unknown_word_id = word_to_id_mappings['<UNK>']\n",
    "    return word_to_id_mappings.get(word,unknown_word_id)\n",
    "\n",
    "# creating training and dev set\n",
    "for idx,paragraph in enumerate(brown.paras()):\n",
    "    for sentence in paragraph:\n",
    "        for i,word in enumerate(sentence):\n",
    "            if i+2 >= len(sentence):\n",
    "                # sentence boundary reached\n",
    "                # ignoring sentence less than 3 words\n",
    "                break\n",
    "            # convert word to id\n",
    "            x_extract = [get_id_of_word(word.lower()),get_id_of_word(sentence[i+1].lower())]\n",
    "            y_extract = [get_id_of_word(sentence[i+2].lower())]\n",
    "            if idx < num_train:\n",
    "                x_train.append(x_extract)\n",
    "                y_train.append(y_extract)\n",
    "            else:\n",
    "                x_dev.append(x_extract)\n",
    "                y_dev.append(y_extract)\n",
    "\n",
    "# making numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_dev = np.array(x_dev)\n",
    "y_dev = np.array(y_dev)  \n",
    "  \n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import multiprocessing\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# Trigram Neural Network Model\n",
    "class TrigramNNmodel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
    "        super(TrigramNNmodel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
    "        self.linear2 = nn.Linear(h, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # compute x': concatenation of x1 and x2 embeddings\n",
    "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
    "        # compute h: tanh(W_1.x' + b)\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # compute W_2.h\n",
    "        out = self.linear2(out)\n",
    "        # compute y: log_softmax(W_2.h)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "--- Creating training and dev dataloaders with 256 batch size ---\n"
     ]
    }
   ],
   "source": [
    "# create parameters\n",
    "gpu = 0 \n",
    "# word vectors size\n",
    "EMBEDDING_DIM = 200\n",
    "CONTEXT_SIZE = 2\n",
    "BATCH_SIZE = 256\n",
    "# hidden units\n",
    "H = 100\n",
    "torch.manual_seed(13013)\n",
    "\n",
    "# check if gpu is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "available_workers = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"--- Creating training and dev dataloaders with {} batch size ---\".format(BATCH_SIZE))\n",
    "train_set = np.concatenate((x_train, y_train), axis=1)\n",
    "dev_set = np.concatenate((x_dev, y_dev), axis=1)\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, num_workers = available_workers)\n",
    "dev_loader = DataLoader(dev_set, batch_size = BATCH_SIZE, num_workers = available_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get accuracy from log probabilities\n",
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "# helper function to evaluate model on dev data\n",
    "def evaluate(model, criterion, dataloader, gpu):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_st = time.time()\n",
    "        for it, data_tensor in enumerate(dataloader):\n",
    "            context_tensor = data_tensor[:,0:2]\n",
    "            target_tensor = data_tensor[:,2]\n",
    "            context_tensor, target_tensor = context_tensor, target_tensor\n",
    "            log_probs = model(context_tensor)\n",
    "            mean_loss += criterion(log_probs, target_tensor).item()\n",
    "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "            count += 1\n",
    "            if it % 500 == 0: \n",
    "                print(\"Dev Iteration {} complete. Mean Loss: {}; Mean Acc:{}; Time taken (s): {}\".format(it, mean_loss / count, mean_acc / count, (time.time()-dev_st)))\n",
    "                dev_st = time.time()\n",
    "\n",
    "    return mean_acc / count, mean_loss / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model Epoch: 1 ---\n",
      "Training Iteration 0 of epoch 0 complete. Loss: 9.500727653503418; Acc:0.0; Time taken (s): 2.103275775909424\n",
      "Training Iteration 500 of epoch 0 complete. Loss: 6.266800403594971; Acc:0.1484375; Time taken (s): 36.90906620025635\n",
      "Training Iteration 1000 of epoch 0 complete. Loss: 6.116858959197998; Acc:0.14453125; Time taken (s): 34.95036220550537\n",
      "Training Iteration 1500 of epoch 0 complete. Loss: 6.026060104370117; Acc:0.1328125; Time taken (s): 36.72751474380493\n",
      "Training Iteration 2000 of epoch 0 complete. Loss: 5.957391738891602; Acc:0.10546875; Time taken (s): 37.28599524497986\n",
      "Training Iteration 2500 of epoch 0 complete. Loss: 6.228488922119141; Acc:0.1484375; Time taken (s): 35.7860791683197\n",
      "Training Iteration 3000 of epoch 0 complete. Loss: 5.779256820678711; Acc:0.19921875; Time taken (s): 37.785094022750854\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 4.968530178070068; Mean Acc:0.19140625; Time taken (s): 1.6621944904327393\n",
      "Dev Iteration 500 complete. Mean Loss: 5.118628631332915; Mean Acc:0.1699569672346115; Time taken (s): 12.884703636169434\n",
      "Epoch 0 complete! Development Accuracy: 0.1691865772008896; Development Loss: 5.132177676874049\n",
      "Best development accuracy improved from 0 to 0.1691865772008896, saving model...\n",
      "\n",
      "--- Training model Epoch: 2 ---\n",
      "Training Iteration 0 of epoch 1 complete. Loss: 6.39088773727417; Acc:0.125; Time taken (s): 1.5402324199676514\n",
      "Training Iteration 500 of epoch 1 complete. Loss: 5.692096710205078; Acc:0.171875; Time taken (s): 37.94850993156433\n",
      "Training Iteration 1000 of epoch 1 complete. Loss: 5.462382793426514; Acc:0.17578125; Time taken (s): 36.55432415008545\n",
      "Training Iteration 1500 of epoch 1 complete. Loss: 5.588323593139648; Acc:0.16796875; Time taken (s): 36.461180210113525\n",
      "Training Iteration 2000 of epoch 1 complete. Loss: 5.542054176330566; Acc:0.1484375; Time taken (s): 36.43138408660889\n",
      "Training Iteration 2500 of epoch 1 complete. Loss: 5.58962345123291; Acc:0.13671875; Time taken (s): 36.67656922340393\n",
      "Training Iteration 3000 of epoch 1 complete. Loss: 5.279577732086182; Acc:0.20703125; Time taken (s): 37.29034948348999\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 4.937081813812256; Mean Acc:0.18359375; Time taken (s): 1.8277127742767334\n",
      "Dev Iteration 500 complete. Mean Loss: 5.0978676716010725; Mean Acc:0.17458832263946533; Time taken (s): 13.048652172088623\n",
      "Epoch 1 complete! Development Accuracy: 0.17377641797065735; Development Loss: 5.110773386674769\n",
      "Best development accuracy improved from 0.1691865772008896 to 0.17377641797065735, saving model...\n",
      "\n",
      "--- Training model Epoch: 3 ---\n",
      "Training Iteration 0 of epoch 2 complete. Loss: 5.985106468200684; Acc:0.1328125; Time taken (s): 1.6790971755981445\n",
      "Training Iteration 500 of epoch 2 complete. Loss: 5.3992085456848145; Acc:0.18359375; Time taken (s): 36.076406955718994\n",
      "Training Iteration 1000 of epoch 2 complete. Loss: 5.251335620880127; Acc:0.1953125; Time taken (s): 37.23606252670288\n",
      "Training Iteration 1500 of epoch 2 complete. Loss: 5.338109970092773; Acc:0.1875; Time taken (s): 35.9044234752655\n",
      "Training Iteration 2000 of epoch 2 complete. Loss: 5.318858623504639; Acc:0.1484375; Time taken (s): 36.52730631828308\n",
      "Training Iteration 2500 of epoch 2 complete. Loss: 5.239077568054199; Acc:0.17578125; Time taken (s): 36.41037392616272\n",
      "Training Iteration 3000 of epoch 2 complete. Loss: 4.970643997192383; Acc:0.234375; Time taken (s): 37.01419687271118\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 4.9384236335754395; Mean Acc:0.1953125; Time taken (s): 1.790203332901001\n",
      "Dev Iteration 500 complete. Mean Loss: 5.13316430207974; Mean Acc:0.17344997823238373; Time taken (s): 13.0433828830719\n",
      "Epoch 2 complete! Development Accuracy: 0.1730162352323532; Development Loss: 5.145047105761135\n",
      "\n",
      "--- Training model Epoch: 4 ---\n",
      "Training Iteration 0 of epoch 3 complete. Loss: 5.685489177703857; Acc:0.13671875; Time taken (s): 1.4214541912078857\n",
      "Training Iteration 500 of epoch 3 complete. Loss: 5.206515789031982; Acc:0.19140625; Time taken (s): 35.468642711639404\n",
      "Training Iteration 1000 of epoch 3 complete. Loss: 5.051833152770996; Acc:0.203125; Time taken (s): 36.202489376068115\n",
      "Training Iteration 1500 of epoch 3 complete. Loss: 5.170336723327637; Acc:0.1953125; Time taken (s): 35.92803478240967\n",
      "Training Iteration 2000 of epoch 3 complete. Loss: 5.161469459533691; Acc:0.1484375; Time taken (s): 36.147160053253174\n",
      "Training Iteration 2500 of epoch 3 complete. Loss: 5.009734153747559; Acc:0.203125; Time taken (s): 36.763890981674194\n",
      "Training Iteration 3000 of epoch 3 complete. Loss: 4.763613700866699; Acc:0.25390625; Time taken (s): 37.37522292137146\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 4.990932941436768; Mean Acc:0.19140625; Time taken (s): 1.7503767013549805\n",
      "Dev Iteration 500 complete. Mean Loss: 5.1853036633032765; Mean Acc:0.17275604605674744; Time taken (s): 13.019173383712769\n",
      "Epoch 3 complete! Development Accuracy: 0.17214307188987732; Development Loss: 5.196190316536847\n",
      "\n",
      "--- Training model Epoch: 5 ---\n",
      "Training Iteration 0 of epoch 4 complete. Loss: 5.476318836212158; Acc:0.15625; Time taken (s): 1.4995849132537842\n",
      "Training Iteration 500 of epoch 4 complete. Loss: 5.057214260101318; Acc:0.1875; Time taken (s): 35.7763729095459\n",
      "Training Iteration 1000 of epoch 4 complete. Loss: 4.864920139312744; Acc:0.19921875; Time taken (s): 36.3898606300354\n",
      "Training Iteration 1500 of epoch 4 complete. Loss: 5.0338640213012695; Acc:0.1953125; Time taken (s): 36.225730419158936\n",
      "Training Iteration 2000 of epoch 4 complete. Loss: 5.036531925201416; Acc:0.15625; Time taken (s): 36.194653272628784\n",
      "Training Iteration 2500 of epoch 4 complete. Loss: 4.873057842254639; Acc:0.2109375; Time taken (s): 36.98461723327637\n",
      "Training Iteration 3000 of epoch 4 complete. Loss: 4.606851577758789; Acc:0.26953125; Time taken (s): 36.9151668548584\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.057142734527588; Mean Acc:0.1796875; Time taken (s): 1.6624042987823486\n",
      "Dev Iteration 500 complete. Mean Loss: 5.239006736321364; Mean Acc:0.1700817048549652; Time taken (s): 13.556675910949707\n",
      "Epoch 4 complete! Development Accuracy: 0.16956572234630585; Development Loss: 5.249377993976369\n"
     ]
    }
   ],
   "source": [
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# create model\n",
    "model = TrigramNNmodel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "\n",
    "# # load it to gpu\n",
    "# model.cuda(gpu)\n",
    "\n",
    "# using ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
    "\n",
    "\n",
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "best_acc = 0\n",
    "best_model_path = None\n",
    "for epoch in range(5):\n",
    "    st = time.time()\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch+1))\n",
    "    for it, data_tensor in enumerate(train_loader):       \n",
    "        context_tensor = data_tensor[:,0:2]\n",
    "        target_tensor = data_tensor[:,2]\n",
    "\n",
    "        context_tensor, target_tensor = context_tensor, target_tensor\n",
    "\n",
    "        # zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get log probabilities over next words\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # calculate current accuracy\n",
    "        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        # backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 500 == 0: \n",
    "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}\".format(it, epoch, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "    print(\"\\n--- Evaluating model on dev data ---\")\n",
    "    dev_acc, dev_loss = evaluate(model, loss_function, dev_loader, gpu)\n",
    "    print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(epoch, dev_acc, dev_loss))\n",
    "    if dev_acc > best_acc:\n",
    "        print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "        best_acc = dev_acc\n",
    "        # set best model path\n",
    "        best_model_path = 'best_model_{}.dat'.format(epoch)\n",
    "        # saving best model\n",
    "        torch.save(model.state_dict(), best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('computer', 'keyboard'): -0.08803991973400116, ('keyboard', 'cat'): 0.12031812220811844, ('dog', 'car'): 0.06517498195171356, ('cat', 'dog'): 0.1584472954273224}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Loading Best Model -------------------\n",
    "best_model = TrigramNNmodel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "# best_model.cuda(gpu)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "lm_similarities = {}\n",
    "\n",
    "# word pairs to calculate similarity\n",
    "words = {('computer','keyboard'),('cat','dog'),('dog','car'),('keyboard','cat')}\n",
    "\n",
    "# ----------- Calculate LM similarities using cosine similarity ----------\n",
    "for word_pairs in words:\n",
    "    w1 = word_pairs[0]\n",
    "    w2 = word_pairs[1]\n",
    "    words_tensor = torch.LongTensor([get_id_of_word(w1),get_id_of_word(w2)])\n",
    "    # words_tensor = words_tensor.cuda(gpu)\n",
    "    # get word embeddings from the best model\n",
    "    words_embeds = best_model.embeddings(words_tensor)\n",
    "    # calculate cosine similarity between word vectors\n",
    "    sim = cos(words_embeds[0],words_embeds[1])\n",
    "    lm_similarities[word_pairs] = sim.item()\n",
    "\n",
    "print(lm_similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write at least one paragraph about what you learned and one paragraph about what questions you have. (also as comment/markdown cell)\n",
    "\n",
    "I actually learned how nerual networks work. Before, I had heard of a lot of these elements of a learning problem, but had no idea what any of it meant or how to make use of any of it. I learned what an activation function is, as well as how backpropogation works. It was really eye-opening to see that backpropogation is just an application of the chain rule. It was also extremely useful to me to walk through the nerual net at the beginning of this homework assignment. It really helps to just see a neural net as a linear combination of nested function.\n",
    "\n",
    "My biggest questions arise from hyperparameters, as well as initial weights. I have no idea how to choose hyperparameters or how to optimize hyperparameters. Along those lines I am curious how to pick initial weights to use, although I guess with a loss function and backpropogation it doesn't really matter where you start. My last question that I have is how to deploy models, but this is more of a Machine Learning Operations question. How do you deploy or load pre-trained models so that you don't have to take an eternity training these neural networks. But honestly, this makes a lot of sense, just curious how someone thought of this. I guess I'm also curious how to tell if your model is overfitting vs underfitting."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2038d1479388d82a9505d662a1f330a1e267339975f16223286a8da7ba6dceaa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

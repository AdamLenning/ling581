{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1 \n",
    "** Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the I am Sam corpus on page 4.\n",
    "\n",
    "$P(w_3 \\mid w_1:w_2) = \\dfrac{C(w_1:w_2 * w_3)}{C(w_1:w_2)} $\n",
    "\n",
    "P(am | \\<s> I) = .5, \n",
    "P(Sam | I am) = .5, \n",
    "P(\\</s> | am Sam) = 1, \n",
    "P(I | \\<s> Sam) = 1, \n",
    "P(am | Sam I) = 1, \n",
    "P(\\</s> | I am) = .5, \n",
    "P(do | \\<s> I) = .5, \n",
    "P(not | I do) = 1, \n",
    "P(like | do not) = 1, \n",
    "P(green | not like) = 1, \n",
    "P(eggs | like green) = 1, \n",
    "P(and | green eggs) = 1, \n",
    "P(ham | eggs and) = 1, \n",
    "P(\\</s> | and ham) = 1\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 \n",
    "** Calculate the probability of the sentence i want chinese food. Give two probabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on page 6, and another using the add-1 smoothed table in Fig. 3.6. Assume the additional add-1 smoothed probabilities P(i|\\<s>) = 0.19 and P(\\</s>|food) = 0.40.\n",
    "\n",
    "## Using Fig 3.2 (unsmoothed):\n",
    "$ 0.25 * 0.33 * 0.0065 * 0.52 * 0.68 = 0.000189618 $\n",
    "\n",
    "## Using Fig 3.6 (smoothed):\n",
    "$ 0.19 * 0.21 * 0.0029 * 0.052 * 0.40 = 0.00000240676 $"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3\n",
    "** Which of the two probabilities you computed in the previous exercise is higher, unsmoothed or smoothed? Explain why.\n",
    "\n",
    "Unsmoothed is higher because you in this sentence all the bigrams are present from the corpus. In a smoothed probability distribution, probability is taken away from the existing bi-grams to allow for unused bi-grams that weren't present in the training corpus."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 \n",
    "** We are given the following corpus, modified from the one in the chapter:\n",
    "    \\<s> I am Sam \\</s>\n",
    "    \\<s> Sam I am \\</s>\n",
    "    \\<s> I am Sam \\</s>\n",
    "    \\<s> I do not like green eggs and Sam \\</s>\n",
    "** Using a bigram language model with add-one smoothing, what is P(Sam | am)? Include \\<s> and \\</s> in your counts just like any other token.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import nltk\n",
    "\n",
    "text = \"\"\"I am Sam. Sam I am. I do not like green eggs and Sam.\"\"\"\n",
    "train, vocab = padded_everygram_pipeline(2, text)\n",
    "\n",
    "model = Laplace(2)\n",
    "model.fit(train, vocab)\n",
    "\n",
    "model.score('Sam', 'am'.split())  # P('is'|'language')\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 \n",
    "Suppose we didn’t use the end-symbol \\</s>. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol \\</s>:\n",
    "\\<s> a b\n",
    "\\<s> b b\n",
    "\\<s> b a\n",
    "\\<s> a a\n",
    "Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet {a,b}is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet {a,b} is also 1.0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "from itertools import product\n",
    "\n",
    "text = [['a', 'b'], ['b', 'b'], ['b', 'a'], ['a', 'a']]\n",
    "padded_sent = []\n",
    "for i in range(len(text)):\n",
    "    padded_sent.extend(list(ngrams(list(pad_sequence(text[i], pad_left=True, left_pad_symbol=\"<s>\", pad_right=False, n=2)), n=2)))\n",
    "\n",
    "counts = {'<s>': {'<s>': 0, 'a': 0, 'b': 0}, 'a': {'<s>': 0, 'a': 0, 'b': 0}, 'b': {'<s>': 0, 'a': 0, 'b': 0}}\n",
    "for tup in padded_sent:\n",
    "    counts[tup[0]][tup[1]] += 1\n",
    "    \n",
    "probs = {'<s>': {'<s>': 0, 'a': 0, 'b': 0}, 'a': {'<s>': 0, 'a': 0, 'b': 0}, 'b': {'<s>': 0, 'a': 0, 'b': 0}}\n",
    "for key in counts:\n",
    "    for key2 in counts[key]:\n",
    "        probs[key][key2] = counts[key][key2] / sum(counts[key].values())\n",
    "\n",
    "prob_sum\n",
    "for tup in list(product(['a', 'b'], repeat=2)):\n",
    "    # P(a|a)* P(a|b) + \n",
    "    prob_sum += "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<s>': {'<s>': 0.0, 'a': 0.5, 'b': 0.5}, 'a': {'<s>': 0.0, 'a': 0.5, 'b': 0.5}, 'b': {'<s>': 0.0, 'a': 0.5, 'b': 0.5}}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('a', 'a'), ('a', 'b'), ('b', 'a'), ('b', 'b')]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6\n",
    "Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains V word types. Express a formula for estimating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2), in terms of various N-gram counts and V. Use the notation c(w1,w2,w3) to denote the number of times that trigram (w1,w2,w3) occurs in the corpus, and so on for bigrams and unigrams."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7\n",
    "We are given the following corpus, modified from the one in the chapter:\n",
    "<s> I am Sam </s>\n",
    "<s> Sam I am </s>\n",
    "<s> I am Sam </s>\n",
    "<s> I do not like green eggs and Sam </s>\n",
    "If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with λ1 = 12 and λ2 =12 , what is P(Sam|am)? Include <s> and </s> in your counts just like any other token."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c3deb8cd993c689b061872ff910ef58aea191cabc6baada44a3fe0a4ea922df"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}